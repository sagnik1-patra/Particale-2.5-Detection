{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9d15f5c-ab9c-4e1e-b723-ffad991d5309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Features: 25\n",
      "Initial scores: [44.29668721856404, 11.295352807083662, 14.693262067128709, 28.863371216882694, 10.507676216885683, 29.735102003259787, 31.796723897542684, 12.684039065923441, 10.31628230402489, 9.802487226422857, 10.866355215658663, 31.688581325934123]\n",
      "\n",
      "=== AIS GENERATION 1/6 ===\n",
      "Scores: [9.810527351785622, 9.892625827737284, 22.900529523862907, 10.876308484076535, 22.476278072819547, 9.183932298765042, 9.670287160273462, 14.888711585032631, 32.98454529030254, 10.591091019885603, 49.1142818989095, 9.659027361727837]\n",
      "\n",
      "=== AIS GENERATION 2/6 ===\n",
      "Scores: [9.210090539827585, 28.956754965167686, 12.245746127865608, 12.114206803419748, 10.84440349925797, 8.923934018301248, 20.817901623023033, 9.648629862573983, 9.123099186384994, 10.63694553663413, 9.337528574935696, 9.66068320181967]\n",
      "\n",
      "=== AIS GENERATION 3/6 ===\n",
      "Scores: [8.89955923352435, 11.05047400854316, 9.010439853267993, 12.977404400515297, 10.813781270986048, 13.850417733116625, 9.44316801780704, 10.290047174819044, 13.7884631854339, 10.266419251334485, 10.535124260138122, 11.722559103553555]\n",
      "\n",
      "=== AIS GENERATION 4/6 ===\n",
      "Scores: [9.20892081349884, 9.190709748281375, 20.96362381019374, 9.42415085463129, 34.10438834079884, 14.670132064704609, 9.06832540240216, 12.982386436890405, 12.15496029233083, 9.309472904771651, 10.416413234898856, 30.923253658001528]\n",
      "\n",
      "=== AIS GENERATION 5/6 ===\n",
      "Scores: [9.067001023966279, 9.398697203870256, 11.005460524445745, 27.34342739293992, 10.432710869390686, 10.552707279598629, 11.681988440902876, 11.567899744112763, 9.320873612775765, 44.33641702264738, 20.279168376696557, 10.134319348497703]\n",
      "\n",
      "=== AIS GENERATION 6/6 ===\n",
      "Scores: [9.063745613491164, 11.132264635699764, 11.636699560333202, 9.972581189889082, 9.634899412364435, 8.998432497587707, 21.98004861060476, 9.210512487843756, 21.539381259042678, 8.938576616600047, 9.434818362042657, 33.089004378801306]\n",
      "\n",
      "BEST MASK SELECTED: [0 0 1 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 0 1]\n",
      "\n",
      "ðŸŽ‰ HYBRID MODEL SAVED SUCCESSFULLY!\n",
      "\n",
      "HYBRID MSE: 8.97711348247488\n",
      "HYBRID R2: 0.8193653292033143\n",
      "\n",
      "ðŸŽ‰ ALL HYBRID FILES SAVED SUCCESSFULLY!\n",
      "Saved in: C:\\Users\\NXTWAVE\\Downloads\\Particale 2.5 Detection\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ“Œ PATHS\n",
    "# ============================================================\n",
    "data_path = r\"C:\\Users\\NXTWAVE\\Downloads\\Particale 2.5 Detection\\archive\\pollution_us_2000_2016.csv\"\n",
    "save_path = r\"C:\\Users\\NXTWAVE\\Downloads\\Particale 2.5 Detection\"\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ“Œ CONFIG\n",
    "# ============================================================\n",
    "target = \"NO2 Mean\"\n",
    "chunk_size = 20000\n",
    "\n",
    "# AIS + PSO parameters\n",
    "POP = 12              # number of feature masks\n",
    "MAX_GEN = 6           # number of AIS generations\n",
    "MASK_MUT_RATE = 0.25  # mutation rate\n",
    "PSO_ITERS = 10        # PSO refinement iterations\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ“Œ LOAD FIRST CHUNK â†’ GET FEATURE NAMES\n",
    "# ============================================================\n",
    "df0 = next(pd.read_csv(data_path, chunksize=chunk_size))\n",
    "\n",
    "df0 = df0.drop(columns=[\"Address\", \"NO2 Units\", \"O3 Units\", \"SO2 Units\", \"CO Units\"])\n",
    "df0[\"Date Local\"] = pd.to_datetime(df0[\"Date Local\"])\n",
    "df0[\"Year\"] = df0[\"Date Local\"].dt.year\n",
    "df0[\"Month\"] = df0[\"Date Local\"].dt.month\n",
    "df0[\"Day\"] = df0[\"Date Local\"].dt.day\n",
    "df0 = df0.drop(columns=[\"Date Local\", target])\n",
    "feature_names = list(df0.columns)\n",
    "\n",
    "DIM = len(feature_names)\n",
    "print(\"Total Features:\", DIM)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ“Œ AIS â€” Generate Random Feature Masks\n",
    "# ============================================================\n",
    "def generate_mask():\n",
    "    return np.random.randint(0, 2, DIM)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ“Œ Evaluate Mask (Streaming training)\n",
    "# ============================================================\n",
    "def evaluate_mask(mask):\n",
    "    model = SGDRegressor(max_iter=5, eta0=0.001)\n",
    "    scaler = MinMaxScaler()\n",
    "    label_enc = {c: LabelEncoder() for c in [\"State\", \"County\", \"City\"]}\n",
    "\n",
    "    first_batch = True\n",
    "    last_X, last_y = None, None\n",
    "\n",
    "    for chunk in pd.read_csv(data_path, chunksize=chunk_size):\n",
    "\n",
    "        chunk = chunk.drop(columns=[\"Address\",\"NO2 Units\",\"O3 Units\",\"SO2 Units\",\"CO Units\"])\n",
    "        chunk = chunk.dropna(subset=[target])\n",
    "\n",
    "        # date\n",
    "        chunk[\"Date Local\"] = pd.to_datetime(chunk[\"Date Local\"])\n",
    "        chunk[\"Year\"] = chunk[\"Date Local\"].dt.year\n",
    "        chunk[\"Month\"] = chunk[\"Date Local\"].dt.month\n",
    "        chunk[\"Day\"] = chunk[\"Date Local\"].dt.day\n",
    "        chunk = chunk.drop(columns=[\"Date Local\"])\n",
    "\n",
    "        # encode\n",
    "        for col in [\"State\",\"County\",\"City\"]:\n",
    "            chunk[col] = label_enc[col].fit_transform(chunk[col].astype(str))\n",
    "\n",
    "        # impute\n",
    "        for col in chunk.columns:\n",
    "            if chunk[col].dtype != \"object\":\n",
    "                chunk[col] = chunk[col].fillna(chunk[col].mean())\n",
    "\n",
    "        y = chunk[target]\n",
    "        X = chunk.drop(columns=[target])\n",
    "\n",
    "        # apply mask\n",
    "        selected_X = X.iloc[:, mask == 1]\n",
    "\n",
    "        if selected_X.shape[1] == 0:\n",
    "            return float(\"inf\")  # invalid mask\n",
    "\n",
    "        # scale\n",
    "        if first_batch:\n",
    "            Xs = scaler.fit_transform(selected_X)\n",
    "            first_batch = False\n",
    "        else:\n",
    "            Xs = scaler.transform(selected_X)\n",
    "\n",
    "        # online train\n",
    "        model.partial_fit(Xs, y)\n",
    "\n",
    "        last_X, last_y = Xs, y\n",
    "\n",
    "    pred = model.predict(last_X)\n",
    "    mse = mean_squared_error(last_y, pred)\n",
    "    return mse\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ“Œ AIS + PSO OPTIMIZATION LOOP\n",
    "# ============================================================\n",
    "population = [generate_mask() for _ in range(POP)]\n",
    "scores = [evaluate_mask(m) for m in population]\n",
    "\n",
    "print(\"Initial scores:\", scores)\n",
    "\n",
    "for gen in range(MAX_GEN):\n",
    "    print(f\"\\n=== AIS GENERATION {gen+1}/{MAX_GEN} ===\")\n",
    "\n",
    "    # Clone best\n",
    "    sorted_idx = np.argsort(scores)\n",
    "    best_mask = population[sorted_idx[0]].copy()\n",
    "\n",
    "    # Mutations\n",
    "    new_pop = [best_mask.copy()]\n",
    "    for i in range(POP - 1):\n",
    "        child = best_mask.copy()\n",
    "        idx = np.random.choice(DIM, int(DIM * MASK_MUT_RATE), replace=False)\n",
    "        child[idx] = 1 - child[idx]\n",
    "        new_pop.append(child)\n",
    "\n",
    "    # Evaluate\n",
    "    new_scores = [evaluate_mask(m) for m in new_pop]\n",
    "\n",
    "    population = new_pop\n",
    "    scores = new_scores\n",
    "    print(\"Scores:\", scores)\n",
    "\n",
    "    # PSO refinement\n",
    "    vel = np.zeros(DIM)\n",
    "    pbest = population[0].copy()\n",
    "\n",
    "    for it in range(PSO_ITERS):\n",
    "        r1, r2 = np.random.rand(), np.random.rand()\n",
    "        vel = 0.5*vel + r1*(pbest - population[0]) + r2*(best_mask - population[0])\n",
    "        population[0] = np.clip(population[0] + vel, 0, 1).round().astype(int)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ“Œ FINAL BEST MASK\n",
    "# ============================================================\n",
    "best_idx = np.argmin(scores)\n",
    "best_mask = population[best_idx]\n",
    "\n",
    "np.save(save_path + r\"\\hybrid_feature_mask.npy\", best_mask)\n",
    "print(\"\\nBEST MASK SELECTED:\", best_mask)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ“Œ FINAL TRAINING USING BEST MASK\n",
    "# ============================================================\n",
    "final_model = SGDRegressor(max_iter=5, eta0=0.001)\n",
    "final_scaler = MinMaxScaler()\n",
    "final_label_enc = {c: LabelEncoder() for c in [\"State\",\"County\",\"City\"]}\n",
    "\n",
    "first = True\n",
    "\n",
    "for chunk in pd.read_csv(data_path, chunksize=chunk_size):\n",
    "\n",
    "    chunk = chunk.drop(columns=[\"Address\",\"NO2 Units\",\"O3 Units\",\"SO2 Units\",\"CO Units\"])\n",
    "    chunk = chunk.dropna(subset=[target])\n",
    "\n",
    "    chunk[\"Date Local\"] = pd.to_datetime(chunk[\"Date Local\"])\n",
    "    chunk[\"Year\"] = chunk[\"Date Local\"].dt.year\n",
    "    chunk[\"Month\"] = chunk[\"Date Local\"].dt.month\n",
    "    chunk[\"Day\"] = chunk[\"Date Local\"].dt.day\n",
    "    chunk = chunk.drop(columns=[\"Date Local\"])\n",
    "\n",
    "    for col in [\"State\",\"County\",\"City\"]:\n",
    "        chunk[col] = final_label_enc[col].fit_transform(chunk[col].astype(str))\n",
    "\n",
    "    for col in chunk.columns:\n",
    "        if chunk[col].dtype != \"object\":\n",
    "            chunk[col] = chunk[col].fillna(chunk[col].mean())\n",
    "\n",
    "    y = chunk[target]\n",
    "    X = chunk.drop(columns=[target])\n",
    "    X = X.iloc[:, best_mask == 1]\n",
    "\n",
    "    if first:\n",
    "        Xs = final_scaler.fit_transform(X)\n",
    "        first = False\n",
    "    else:\n",
    "        Xs = final_scaler.transform(X)\n",
    "\n",
    "    final_model.partial_fit(Xs, y)\n",
    "\n",
    "    last_X, last_y = Xs, y\n",
    "\n",
    "\n",
    "# Save hybrid model\n",
    "joblib.dump(final_model, save_path + r\"\\hybrid_model.pkl\")\n",
    "joblib.dump(final_scaler, save_path + r\"\\hybrid_scaler.pkl\")\n",
    "joblib.dump(final_label_enc, save_path + r\"\\hybrid_label_encoders.pkl\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ HYBRID MODEL SAVED SUCCESSFULLY!\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ“Œ EVALUATION\n",
    "# ============================================================\n",
    "pred = final_model.predict(last_X)\n",
    "errors = abs(last_y - pred)\n",
    "\n",
    "mse = mean_squared_error(last_y, pred)\n",
    "r2 = r2_score(last_y, pred)\n",
    "\n",
    "print(\"\\nHYBRID MSE:\", mse)\n",
    "print(\"HYBRID R2:\", r2)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ“Œ SAVE RESULTS\n",
    "# ============================================================\n",
    "res_df = pd.DataFrame({\"Actual\": last_y.values, \"Predicted\": pred})\n",
    "res_df.to_csv(save_path + r\"\\hybrid_pollution_results.csv\", index=False)\n",
    "\n",
    "with open(save_path + r\"\\hybrid_pollution_predictions.json\",\"w\") as f:\n",
    "    json.dump({\n",
    "        \"actual\": last_y.values.tolist(),\n",
    "        \"predicted\": pred.tolist(),\n",
    "        \"mse\": float(mse),\n",
    "        \"r2\": float(r2)\n",
    "    }, f, indent=4)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ“Œ SAVE GRAPHS\n",
    "# ============================================================\n",
    "\n",
    "# ERROR TREND\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(errors[:1000])\n",
    "plt.title(\"Hybrid Error Trend\")\n",
    "plt.grid()\n",
    "plt.savefig(save_path + r\"\\hybrid_error_trend.png\")\n",
    "plt.close()\n",
    "\n",
    "# SCATTER\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(last_y[:1000], pred[:1000], alpha=0.3)\n",
    "plt.title(\"Hybrid Scatter Plot\")\n",
    "plt.grid()\n",
    "plt.savefig(save_path + r\"\\hybrid_scatter.png\")\n",
    "plt.close()\n",
    "\n",
    "# COMPARISON\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(last_y.values[:300], label=\"Actual\")\n",
    "plt.plot(pred[:300], label=\"Predicted\")\n",
    "plt.title(\"Hybrid Comparison\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(save_path + r\"\\hybrid_comparison.png\")\n",
    "plt.close()\n",
    "\n",
    "# RESIDUAL\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(errors, bins=40, color='orange')\n",
    "plt.title(\"Hybrid Residual Distribution\")\n",
    "plt.grid()\n",
    "plt.savefig(save_path + r\"\\hybrid_residual.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nðŸŽ‰ ALL HYBRID FILES SAVED SUCCESSFULLY!\")\n",
    "print(\"Saved in:\", save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e45a5-0f9c-4fa8-8765-fe312fec17b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
