{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52cdaacd-6835-4550-8fc0-7b0a339527e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== COLUMN NAMES IN DATASET ======\n",
      "\n",
      "Index(['Unnamed: 0', 'State Code', 'County Code', 'Site Num', 'Address',\n",
      "       'State', 'County', 'City', 'Date Local', 'NO2 Units', 'NO2 Mean',\n",
      "       'NO2 1st Max Value', 'NO2 1st Max Hour', 'NO2 AQI', 'O3 Units',\n",
      "       'O3 Mean', 'O3 1st Max Value', 'O3 1st Max Hour', 'O3 AQI', 'SO2 Units',\n",
      "       'SO2 Mean', 'SO2 1st Max Value', 'SO2 1st Max Hour', 'SO2 AQI',\n",
      "       'CO Units', 'CO Mean', 'CO 1st Max Value', 'CO 1st Max Hour', 'CO AQI'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = r\"C:\\Users\\NXTWAVE\\Downloads\\Particale 2.5 Detection\\archive\\pollution_us_2000_2016.csv\"\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(\"\\n====== COLUMN NAMES IN DATASET ======\\n\")\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecb2973d-f8d2-48f1-9406-cf099467e930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Columns ===\n",
      "Index(['Unnamed: 0', 'State Code', 'County Code', 'Site Num', 'Address',\n",
      "       'State', 'County', 'City', 'Date Local', 'NO2 Units', 'NO2 Mean',\n",
      "       'NO2 1st Max Value', 'NO2 1st Max Hour', 'NO2 AQI', 'O3 Units',\n",
      "       'O3 Mean', 'O3 1st Max Value', 'O3 1st Max Hour', 'O3 AQI', 'SO2 Units',\n",
      "       'SO2 Mean', 'SO2 1st Max Value', 'SO2 1st Max Hour', 'SO2 AQI',\n",
      "       'CO Units', 'CO Mean', 'CO 1st Max Value', 'CO 1st Max Hour', 'CO AQI'],\n",
      "      dtype='object')\n",
      "\n",
      "MSE: 1.5881062684533043\n",
      "R2: 0.9824936224453027\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "\n",
      "âœ… All model files saved in: C:\\Users\\NXTWAVE\\Downloads\\Particale 2.5 Detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import yaml\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ðŸ“Œ PATHS\n",
    "# ---------------------------------------------------------\n",
    "data_path = r\"C:\\Users\\NXTWAVE\\Downloads\\Particale 2.5 Detection\\archive\\pollution_us_2000_2016.csv\"\n",
    "save_path = r\"C:\\Users\\NXTWAVE\\Downloads\\Particale 2.5 Detection\"\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ðŸ“Œ LOAD DATA\n",
    "# ---------------------------------------------------------\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(\"\\n=== Columns ===\")\n",
    "print(df.columns)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ðŸ“Œ REMOVE STRING COLUMNS THAT CAUSE ERRORS\n",
    "# ---------------------------------------------------------\n",
    "drop_columns = [\n",
    "    \"Address\",\n",
    "    \"NO2 Units\",\n",
    "    \"O3 Units\",\n",
    "    \"SO2 Units\",\n",
    "    \"CO Units\"\n",
    "]\n",
    "\n",
    "df = df.drop(columns=drop_columns)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ðŸ“Œ TARGET (you can change to O3 Mean / SO2 Mean / CO Mean)\n",
    "# ---------------------------------------------------------\n",
    "target = \"NO2 Mean\"\n",
    "\n",
    "# Remove rows with missing target\n",
    "df = df.dropna(subset=[target])\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ðŸ“Œ DATE PROCESSING\n",
    "# ---------------------------------------------------------\n",
    "df[\"Date Local\"] = pd.to_datetime(df[\"Date Local\"])\n",
    "df[\"Year\"] = df[\"Date Local\"].dt.year\n",
    "df[\"Month\"] = df[\"Date Local\"].dt.month\n",
    "df[\"Day\"] = df[\"Date Local\"].dt.day\n",
    "df = df.drop(columns=[\"Date Local\"])\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ðŸ“Œ LABEL ENCODE CATEGORICAL COLUMNS\n",
    "# ---------------------------------------------------------\n",
    "label_cols = [\"State\", \"County\", \"City\"]\n",
    "encoders = {}\n",
    "\n",
    "for col in label_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    encoders[col] = le\n",
    "\n",
    "joblib.dump(encoders, save_path + r\"\\label_encoders.pkl\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ðŸ“Œ SPLIT DATA\n",
    "# ---------------------------------------------------------\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ðŸ“Œ SCALING (Now only numeric â†’ no error)\n",
    "# ---------------------------------------------------------\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "joblib.dump(scaler, save_path + r\"\\scaler.pkl\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ðŸ“Œ TRAIN/TEST SPLIT\n",
    "# ---------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ðŸ“Œ MODEL TRAINING\n",
    "# ---------------------------------------------------------\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=20,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"\\nMSE:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"R2:\", r2_score(y_test, y_pred))\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ðŸ“Œ SAVE MODELS\n",
    "# ---------------------------------------------------------\n",
    "joblib.dump(model, save_path + r\"\\pollution_model.pkl\")\n",
    "\n",
    "# JSON\n",
    "model_json = {\n",
    "    \"model\": \"RandomForestRegressor\",\n",
    "    \"target\": target,\n",
    "    \"features\": list(X.columns)\n",
    "}\n",
    "with open(save_path + r\"\\pollution_model.json\", \"w\") as f:\n",
    "    json.dump(model_json, f, indent=4)\n",
    "\n",
    "# YAML\n",
    "with open(save_path + r\"\\pollution_model.yaml\", \"w\") as f:\n",
    "    yaml.dump(model_json, f)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ðŸ“Œ H5 KERAS WRAPPER\n",
    "# ---------------------------------------------------------\n",
    "input_layer = keras.Input(shape=(X_train.shape[1],))\n",
    "output_layer = keras.layers.Dense(1)(input_layer)\n",
    "keras_model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "keras_model.save(save_path + r\"\\pollution_model.h5\")\n",
    "\n",
    "print(\"\\nâœ… All model files saved in:\", save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0195f5f3-0338-42ca-b4d7-aa0a4d6eaf13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
